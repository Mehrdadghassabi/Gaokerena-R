% !TeX root=../main.tex
\chapter{
	معرفی مدل زبانی گائوکرنا-\lr{V}
}
\section{
مقدمه
}
در این فصل با استفاده از دادگانی که گردآوری کرده ایم یک مدل پایه
\footnote{\lr{baseline model}}
را تمرین میدهیم تا مدل جدید
مدل گائوکرنا-\lr{V}
را معرفی کنیم که همانطور که در فصل نخست درباره آن صحبت شد دارای توانایی استدلال نیست.

در ادامه با استفاده از ترجمه قسمت پزشکی مجموعه داده
\lr{MMLU}
به مقایسه مدل جدید خود با مدل پایه و بقیه جایگزین ها خواهیم پرداخت.

\section{مدل پایه}
به دلیل عدم وجود یک مدل زبانی پزشکی فارسی متن باز
\footnote{\lr{open source}}
ما مجبور به انتخاب یک مدل زبانی همه منظوره
\footnote{\lr{general purpose}}
به عنوان مدل پایه هستیم.

گزینه‌های متعددی مانند
\lr{qwen2.5}
\cite{b56}
، 
\lr{aya-expanse}
\cite{b57}
، 
\lr{gemma2}
\cite{b58}
و
\lr{PersianMind}
\cite{b59}
برای انتخاب در دسترس بودند، که به دو دلیل زیر مدل
\lr{aya-expanse-8b}
انتخاب شده است.

دلیل نخست این است که داده‌های آموزشی سایر مدل‌ها عمدتا شامل زبان‌های غیر فارسی هستند، که این امر می‌تواند منجر به ایجاد سوگیری‌هایی در مدل شود که حتی در صورت دستور صریح به استفاده از زبان فارسی، باعث تولید کاراکترهای غیر فارسی می‌شود. در مقابل،
\lr{aya-expanse}
درک قوی‌ای از دستور زبان فارسی نشان می‌دهد و متنی غنی و دستوری صحیح به زبان فارسی تولید می‌کند، که آن را به گزینه‌ای بهتر برای پژوهش ما تبدیل می‌کند.

علاوه بر این، اگر ما پارامترهای به‌روزرسانی‌شده خود را به جای
\lr{aya-expanse}
در مدل دیگری از خانواده
\lr{aya}
، یعنی
\lr{aya-vision}
\cite{b60}
ادغام کنیم، این امکان را به دست می‌آوریم که تصاویر پزشکی مانند
\lr{MRI}
و 
\lr{CT scan}
را به عنوان ورودی بپذیریم. این امر باعث افزایش قابلیت کاربرد مدل ما در حوزه پزشکی خواهد شد.

\subsection{ویژگی های مدل
\lr{aya-expanse}
}
همانطور که پیشتر از آن یاد شد مدل زبانی 
\lr{aya-expanse}
قابلیت تولید متن های فارسی غنی با دستور زبانی صحیح را داراست
\footnote{
به جز زبان فارسی این مدل از بیست و دو زبان دیگر نیز پشتیبانی میکند.
}
 ویژگی که بقیه مدل های زبانی تنها برای برخی از زبان های دارای منابع غنی
 \footnote{\lr{resource-rich languages}}
 مانند انگلیسی و چینی دارا هستند.
 
 این ویژگی زمانی قابل دسترس است که علاوه بر در دسترس بودن منابع غنی برای تمرین منابع به صورت مساوی بین زبان هایی که مدل زبانی پشتیبانی میکند تقسیم شده باشد؛
\lr{aya-expanse} 
با تولید دادگان مصنوعی
 \footnote{\lr{synthetic data}}
 بر مشکل نبود دادگان فائق آمده است، البته تمرین دادن یک مدل زبانی با دادگان مصنوعی تولید شده توسط یک مدل زبانی دیگر باعث فروپاشی مدل
  \footnote{\lr{model collapse}}
  \cite{b61}
میشود که 
\lr{aya-expanse}
با معرفی مکانیسم آربیتاژ داده
\cite{b62}
 که در شکل
\ref{fig4}
 میبینید از رخ دادن این اتفاق جلوگیری کرده است.
 
\begin{figure}[ht]
	\centerline{\includegraphics[width=0.9\textwidth]{fig4}}
	\caption{
	  مکانیسم آربیتاژ داده
	}
	\label{fig4}
\end{figure}
 
\section{
تنظیم دقیق روی پیکره پزشکی
}
برای دستیابی به مدل
مدل گائوکرنا-\lr{V}
همان‌طور که در نمودار کاهش خطا در تصویر
\ref{fig12}
 میبینید ابتدا مدل پایه روی شصت درصد از پیکره پزشکی گردآوری شده تنظیم دقیق
\footnote{\lr{fine tune}}
شده است
\footnote{
برای دسترسی به کد تنظیم دقیق مدل پایه روی پیکره پزشکی گرد آوری شده میتوانید به آدرس
\lr{https://github.com/Mehrdadghassabi/Gaokerena-V/blob/main/fine-tuning/Pretraining.ipynb}
مراجعه کنید
}
،برای این کار از اندازه دسته
\footnote{\lr{batch size}}
برابر با دو استفاده کردیم تا نیاز به حافظه در طول آموزش کاهش یابد. علاوه بر این، از تجمع گرادیان
\footnote{\lr{gradient accumulation}}
با شانزده مرحله استفاده کردیم که به طور مؤثر اندازه‌ی کلی دسته را به سی و دو افزایش داد و دینامیک پایدار آموزش را فراهم کرد.

برای کاهش بیشتر مصرف حافظه در فرآیند تنظیم دقیق، از روش
\lr{LoRA}
\cite{b63}
\footnote{\lr{low rank adaptation}}
بهره بردیم تا تعداد پارامترهای قابل آموزش به‌طور چشمگیری کاهش یابد. برای این کار از رتبه
\footnote{\lr{rank}}
برابر با هشت، مقدار آلفا برابر با شانزده، نرخ حذف
\footnote{\lr{drop out}}
برابر با پنج درصد و نرخ پوسیدگی وزن
\footnote{\lr{weight decay}}
 ده درصد استفاده کرده و وزن‌های
\lr{LoRA}
را به تمام پارامترهای قابل آموزش در هر لایه ترنسفورمر اختصاص دادیم.

برای بهینه‌سازی بیشتر این فرآیند، از تکنیک‌های کارآمد توکن‌سازی
\footnote{\lr{efficient tokenization}}
و  تکنیک های آموزش مبتنی بر مدیریت حافظه
\footnote{\lr{memory-aware training techniques}}
بهره بردیم. فرآیند توکن‌سازی متن ورودی را به دنباله‌های قابل مدیریت توکن تقسیم کرد، و با کوتاه کردن، پر کردن، و مدیریت توکن‌های اضافی، ساختار ورودی و برچسب‌ها را ثابت نگه داشت تا یکپارچگی مفهومی در طول ثابت زمینه حفظ شود. این آماده‌سازی ساده‌شده، همراه با تنظیم دقیق مبتنی بر 
\lr{LoRA}
،وزاستفاده از
\lr{Flash Attention 2}
\cite{b64}
بهبود یافت.

\lr{Flash Attention 2}
با کاهش سربار حافظه، امکان مدیریت طول‌های زمینه بلندتر و اندازه‌های دسته بزرگ‌تر را به صورت کارآمد فراهم میکند، که به تنظیم دقیق موثر کمک کرده و تعادل بین کارایی محاسباتی و عملکرد مدل را برقرار میسازد.

\begin{figure}[ht]
	\centerline{\includegraphics[width=0.9\textwidth]{fig12}}
	\caption{
	  نمودار کاهش خطا تنظیم دقیق روی پیکره پزشکی
	}
	\label{fig12}
\end{figure}

\section{
تنظیم دستورالعملی روی مجموعه داده
\lr{MF3QA}
}
پس از تنظیم دقیق روی پیکره پزشکی، بایستی فرآیند تنظیم دستورالعملی
\footnote{\lr{instruction tuning}}
را با استفاده از مجموعه داده
\lr{MF3QA}
که فصل پیشین معرفی شد انجام دهیم.
\footnote{
	برای دسترسی به کد تنظیم دستورالعملی روی مجموعه داده 
	\lr{MF3QA}
	 میتوانید به آدرس
	\lr{https://github.com/Mehrdadghassabi/Gaokerena-V/blob/main/fine-tuning/InstructionTuning.ipynb}
	مراجعه کنید
}
 همان‌طور که در نمودار کاهش خطا در تصویر
 \ref{fig13}
 می‌بینید برای این کار به طور مشخص، باز هم از روش
\lr{LoRA}
استفاده کرده ایم ولی این بار با رتبه
\footnote{\lr{rank}}
برابر با دو، آلفا برابر با دو، نرخ حذف برابر با چهل درصد و نرخ پوسیدگی وزن
\footnote{\lr{weight decay}}
پنجاه درصد استفاده کرده ایم. فرآیند تنظیم دستورالعمل تنها برای یک دوره
\footnote{\lr{epoch}}
انجام شد تا مدل شیوه پاسخگویی درست را بهتر درک کند.

\begin{figure}[ht]
	\centerline{\includegraphics[width=0.9\textwidth]{fig13}}
	\caption{
	  نمودار کاهش خطا تنظیم دقیق روی مجموعه داده
	  \lr{MF3QA}
	}
	\label{fig13}
\end{figure}

\section{
رد پای کربن
مدل گائوکرنا-\lr{V}
}
اثر کربنی حاصل از بهینه‌سازی
مدل گائوکرنا-\lr{V}
که شامل مراحل تنظیم دقیق و تنظیم دستورالعملی میشود، بر اساس مشخصات سخت‌افزاری و مدت زمان اجرا تخمین زده شده است. فرآیند تمرین به مدت نوزده ساعت بر روی کارت‌ گرافیک
\lr{NVIDIA A100 PCIe 40GB}
که در منطقه شرق آسیا
\footnote{\lr{asia-east1}}
پلتفرم ابری گوگل میزبانی می‌شدند، اجرا شده است.

با توجه به مصرف برق معمولی هر پردازنده گرافیکی که برابر با دویست و پنجاه وات است، کل انرژی مصرف‌شده در این مدت برابر
\lr{4.750}
کیلووات‌ساعت است.
با در نظر گرفتن ضریب شدت کربن شبکه‌ شرق آسیا که برابر با
\lr{560}
گرم کربن دی اکسید معادل به ازای هر کیلووات‌ساعت است، میزان انتشار کربن در طول این فرآیند به
\lr{2660}
گرم می‌رسد.
\cite{b65}

\section{
نتایج
}
در نبود مدل های پزشکی فارسی برای مقایسه ما
مدل گائوکرنا-\lr{V}
را با مدل های زبانی فارسی همه منظوره
\footnote{\lr{general purpose language models}}
و جایگزین های خط لوله ای
\footnote{\lr{pipeline alternatives}}
مقایسه کرده ایم.

همانطور که در شکل 
\ref{fig5}
میبینید
جایگزین های خط لوله ای شامل یک سری مراحل است ابتدا، یک مترجم پرسش کاربر را از فارسی به انگلیسی تبدیل می‌کند، سپس این پرسش انگلیسی به یک مدل زبانی پزشکی انگلیسی داده می‌شود، و در نهایت، پاسخ تولیدشده توسط مدل انگلیسی دوباره از انگلیسی به فارسی ترجمه می‌شود.

\begin{figure}[ht]
	\centerline{\includegraphics[width=0.9\textwidth]{fig5}}
	\caption{
		مکانیسم جایگزین خط لوله ای 
	}
	\label{fig5}
\end{figure}
\subsection{
	مقایسه با مدل های زبانی فارسی همه منظوره
}
همان‌طور که در جدول
\ref{tab:model_results_vs_general_purpose_languages}
مشاهده می‌کنید،
مدل گائوکرنا-\lr{V}
توانست با موفقیت کنکور علوم پایه پزشکی شهریور
\lr{1402}
، را پشت سر بگذارد
\footnote{
		نمره قبولی در این امتحان سی و شش درصد بدون نمره منفی بوده است.
}
و به اولین مدل زبان فارسی با کمتر از هشت میلیارد پارامتر تبدیل شد که این آزمون را با موفقیت پشت سر گذاشته است. علاوه بر این، مدل ما در قسمت پزشکی مجموعه داده
\footnote{\lr{MMLU}}
نیز بهبودهایی را نشان داد و نه تنها میانگین نمرات بالاتری کسب کرد، بلکه در اکثر زیرشاخه‌ها عملکرد برجسته‌ای داشت و توانایی خود را در درک و تولید دانش پزشکی به زبان فارسی به نمایش گذاشت.

قابل توجه است که دلیل سرعت بالای پاسخگویی 
\lr{PersianMind}
، همان‌طور که در جدول
\ref{tab:model_results_vs_general_purpose_languages}
نشان داده شده، این است که این مدل تمایل دارد پاسخ‌های بسیار کوتاه‌تری نسبت به مدل‌های دیگر تولید کند و توکن پایان پاسخ را زودتر ایجاد نماید. علاوه بر ارزیابی پرسش و پاسخ چندگزینه‌ای، ما از
\lr{GPT-4o}
\cite{b66}
به عنوان داور برای پرسش و پاسخ آزاد نیز استفاده کردیم. قسمت تست مجموعه داده 
\lr{MF3QA}
به مدل زبان رقیب و مدل ما ارائه شد. همان‌طور که در شکل
\ref{fig6}
نشان داده شده است، 
\lr{GPT-4o}
به طور عمده پاسخ‌های تولیدشده توسط مدل ما را نسبت به سه مدل زبان دیگر ترجیح داده است.

\begin{figure}[ht]
	\centerline{\includegraphics[width=0.9\textwidth]{fig6}}
	\caption{
		نرخ پیروزی گائوکرنا-\lr{V}
	در رقابت با بقیه مدل های زبانی فارسی همه منظوره
	}
	\label{fig6}
\end{figure}

\begin{table}[ht]
	\centering
	\begin{tabular}{|l|c|c|c|c|}  % Using vertical lines for a simple table
		\hline
		\textbf{} & \textbf{Gaokerena-V} & \textbf{\lr{aya-expanse-8b}} & \textbf{\lr{Qwen2.5}} & \textbf{\lr{PersianMind}} \\
		& \lr{(ours)} & \lr{(baseline)} &  &  \\ \hline
		\lr{MMLU-} & \textbf{\lr{48.14}} & \lr{40.74} & \lr{41.48} & \lr{25.18} \\ 
		\lr{anatomy(fa)} &  &  &  &  \\ \hline
		\lr{MMLU-} &  &  &  &  \\
		\lr{medical} & \textbf{\lr{53.0}} & \lr{49.0} & \lr{52.0} & \lr{34.0} \\ 
		\lr{genetics(fa)} &  &  &  &  \\ \hline
		\lr{MMLU-} &  &  &  &  \\
		\lr{college} & \lr{43.93} & \textbf{\lr{44.51}} & \lr{43.35} & \lr{20.23} \\
		\lr{medicine(fa)} &  &  &  &  \\ \hline
		\lr{MMLU-} &  &  &  &  \\
		\lr{clinical}& \textbf{\lr{55.47}} & \lr{52.07} & \lr{47.92} & \lr{25.28} \\
		\lr{knowledge(fa)}&  &  &  &  \\ \hline
		\lr{MMLU-} &  &  &  &  \\
		\lr{professional}& \textbf{\lr{47.05}} & \lr{45.58} & \lr{43.01} & \lr{23.89} \\ 
		\lr{medicine(fa)}&  &  &  &  \\ \hline
		\lr{MMLU-} &  &  &  &  \\
		\lr{college}& \textbf{\lr{47.22}} & \lr{45.14} & \lr{44.85} & \lr{32.63} \\
		\lr{biology(fa)}&  &  &  &  \\ \hline
		\lr{MMLU(avg)} & \textbf{\lr{49.31}} & \lr{46.64} & \lr{45.17} & \lr{25.89} \\ \hline
		\lr{IBMSEE Sept} &  &  &  &  \\ 
		\lr{2023} & \textbf{\lr{38.69}} & \lr{34.52} & \lr{33.33} & \lr{19.64} \\  \hline
		\lr{Number of}&  &  &  &  \\
		\lr{parameters} & \lr{8b} & \lr{8b} & \lr{7.6b} & \lr{6.8b} \\ \hline
		\lr{inference time} &حدود \lr{10s} & حدود \lr{8s} & حدود \lr{15s} & حدود \lr{2s} \\  \hline
	\end{tabular}
    \caption{مقایسه مدل گائوکرنا-\lr{V}
    با بقیه مدل های زبانی فارسی همه منظوره
    }
	\label{tab:model_results_vs_general_purpose_languages}
\end{table}

\subsection{
	مقایسه با جایگزین های خط لوله ای
}
همان‌طور که قبلا اشاره شد، یکی از گزینه‌های جایگزین برای توسعه یک مدل زبانی پزشکی فارسی، استفاده از جایگزین های خط لوله ای
\footnote{\lr{pipeline alternatives}}
است. با این حال، یکی از مشکلات عمده این سیستم‌ها سرعت پایین آن‌ها است. این سیستم‌ها زمان استنتاج بالایی دارند، زیرا خروجی یک مدل باید به مدل دوم منتقل شود و سپس خروجی مدل دوم دوباره توسط مدل اول پردازش شود. این فرآیند تکراری به طور قابل توجهی کارایی سیستم را کاهش می‌دهد.

برای رفع مشکل سرعت پایین سیستم‌های مرحله‌ای، ما تمام پارامترها شامل پارامترهای مربوط به مترجم‌ها و مدل زبانی پزشکی را به طور همزمان بارگذاری کرده‌ایم. آزمایش‌های ما با مدل‌هایی مانند
\lr{Medmobile}
همراه با
\lr{gemma-2b-it}
به عنوان مترجم‌، و 
\lr{Medmobile}
همراه با مدل‌های
\lr{parsinlu}
\cite{b67}
\cite{b68}
به عنوان مترجم‌ انجام گرفته است؛ همان‌طور که در جدول 
\ref{tab:model_results_vs_pipeline_alternative}
مشاهده میکنید جایگزین های خط لوله ای دقت و سرعت بسیار پایینی از خود نشان دادند.

یکی دیگر از مشکلات مهم جایگزین های خط لوله ای، عملکرد ضعیف آن‌ها در شناسایی و ترجمه دقیق اصطلاحات پزشکی است. این محدودیت چالشی جدی ایجاد می‌کند، زیرا دقت در استفاده از اصطلاحات تخصصی برای ارتباط موثر در محیط‌های مراقبت‌های بهداشتی حیاتی است. علت اصلی این ضعف احتمالاً به این دلیل است که مترجم‌های استفاده‌شده در این سیستم‌ها به طور خاص برای ترجمه پزشکی توسعه نیافته‌اند. برخلاف مدل‌های ترجمه عمومی، ترجمه پزشکی نیازمند درک دقیق واژگان تخصصی، زمینه و پیچیدگی‌های زبان پزشکی است.

در حال حاضر، هیچ مدلی برای ترجمه پزشکی به زبان فارسی طراحی نشده است، که این امر باعث می‌شود سیستم‌های موجود توانایی کافی برای مدیریت پیچیدگی‌های اصطلاحات پزشکی نداشته باشند. همان‌طور که در شکل 
\ref{fig7}
نشان داده شده است، این محدودیت‌ها منجر به نرخ پیروزی بسیار پایین جایگزین های خط لوله ای در رقابت با 
مدل گائوکرنا-\lr{V}
شده است.
\begin{figure}[ht]
	\centerline{\includegraphics[width=0.9\textwidth]{fig7}}
	\caption{
		نرخ پیروزی گائوکرنا-\lr{V}
		در رقابت با جایگزین های خط لوله ای
	}
	\label{fig7}
\end{figure}

\begin{table}[ht]
	\centering
	\begin{tabular}{|l|c|c|c|}  % Using vertical lines for a simple table
		\hline
		\textbf{} & \textbf{Gaokerena} 
		& \lr{\textbf{MedMobile} + \textbf{gemma2}} & \textbf{MedMobile} \\ 
		& \lr{(ours)} & \lr{\textbf{-2b-it}} & \textbf{+ parsinlu} \\ \hline
		\lr{MMLU-} &  &  &  \\ 
		\lr{anatomy(fa)}  & \textbf{\lr{48.14}} & \lr{14.07} & \lr{25.18}  \\ \hline
		\lr{MMLU-} &    &  &  \\
		\lr{medical-genetics(fa)} & \textbf{\lr{53.0}} & \lr{20.0} & \lr{35.0} \\ \hline
		\lr{MMLU-} &  &    &  \\
		\lr{college-medicine(fa)} & \textbf{\lr{43.93}} & \lr{19.08} & \lr{27.17} \\ \hline
		\lr{MMLU-} &    &  &  \\
		\lr{clinical-knowledge(fa)}& \textbf{\lr{55.47}} & \lr{27.54} & \lr{31.70} \\ \hline
		\lr{MMLU-} &  &  &  \\
		\lr{professional-medicine(fa)}& \textbf{\lr{47.05}} & \lr{17.27} & \lr{33.82} \\ \hline
		\lr{MMLU-} &  &  &  \\
		\lr{college-biology(fa)}& \textbf{\lr{47.22}} & \lr{18.75} & \lr{31.25} \\ \hline
		\lr{MMLU(avg)} & \textbf{\lr{49.31}} & \lr{20.11} & \lr{30.99} \\ \hline
		\lr{IBMSEE Sept2023} & \textbf{\lr{38.69}}  & \lr{24.40} & \lr{32.73}  \\ \hline
		\lr{Number of parameters} & \lr{8b} & \lr{3.8b+2b} & \lr{3.8b+1.2b+1.2b} \\ \hline
		\lr{inference time} &حدود \lr{10s} &حدود \lr{20s} & حدود \lr{30s} \\  \hline
	\end{tabular}
		\caption{مقایسه مدل گائوکرنا-\lr{V}
		با جایگزین های خط لوله ای
	}
	\label{tab:model_results_vs_pipeline_alternative}
\end{table}
