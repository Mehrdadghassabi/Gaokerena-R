% !TeX root=../main.tex
\chapter{ادبیات موضوع}
\section{مقدمه}
در این فصل به بررسی مفاهیم مدل‌های زبانی 
\footnote{\lr{language models}}
انواع آن و چگونگی سنجش‌ آن‌ها خواهیم پرداخت. همچنین، انواع سیستم‌های پرسش و پاسخ
\footnote{\lr{question answering systems}}
 را مورد تحلیل قرار خواهیم داد تا درک بهتری از عملکرد و کاربردهای مختلف این سیستم‌ها به دست آوریم.
\section{مدل های زبانی}
مدل‌های زبانی ابزارهای پیشرفته‌ای هستند که برای پردازش و تولید زبان طبیعی طراحی شده‌اند. این مدل‌ها با استفاده از یادگیری ماشین
\footnote{\lr{machine learning}}
 و به ویژه یادگیری عمیق
 \footnote{\lr{deep learning}}
 ، توانایی درک و تولید متن را دارند.

در حقیقت، وظیفه مدل‌های زبانی پیش بینی توکن بعدی
 \footnote{\lr{next token prediction}}
 بر اساس متنی است که تاکنون تولید شده است.
 
 این مدل ها دارای انواع مختلفی است که در ادامه آنها را بررسی میکنیم.
\subsection{مدل های زبانی آماری}
مدل‌های زبانی
\lr{n-gram}
ابتدایی ترین مدل های زبانی هستند که در دهه نود میلادی به عنوان جایگزینی برای ترجمه ماشینی مبتنی بر قانون
\footnote{\lr{rule-based machine translation}}
معرفی شدند
\cite{b2}
همانطور که در فرمول 
\ref{ngram-formula}
مشاهده میکنید این مدل‌ها برای پیش‌بینی توکن بعدی از فراوانی 
\lr{n-gram}
ها در پیکره
\footnote{\lr{corpus}}
موجود استفاده میکنند. به عنوان مثال در مدل زبانی
\lr{2-gram}
، احتمال وقوع یک کلمه تنها بر اساس کلمه قبلی محاسبه می‌شود.

\begin{equation*}
	\label{ngram-formula}
	P(w_1, w_2, ..., w_n) = \prod_{i=1}^{n} P(w_i | w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})
\end{equation*}



\subsection{مدل های زبانی بازگشتی}
 شبکه‌های عصبی بازگشتی نوعی از شبکه‌های عصبی مصنوعی 
 \footnote{\lr{artificial neural networks}} 
 هستند که به طور خاص برای پردازش توالی‌ها 
 \footnote{\lr{sequences}}
  طراحی شده‌اند.
  \cite{b3}
   از آنجا که زبان را نیز می‌توان به صورت یک توالی از توکن‌ها تعریف کرد، بنابراین می‌توان از شبکه‌های عصبی بازگشتی به عنوان مدل‌های زبانی استفاده کرد.
  
  همانطور که در فرمول 
  \ref{rnn-formula}
  که فرمول پایه شبکه های عصبی است می بینید ویژگی اصلی شبکه‌های عصبی بازگشتی این است که دارای حلقه‌های بازگشتی است که به آن اجازه می‌دهد اطلاعات را از مراحل قبلی به مراحل بعدی منتقل کند. این ساختار باعث می‌شود شبکه‌های عصبی بازگشتی بتواند وابستگی‌های زمانی و ترتیبی داده‌ها را مدل‌سازی کند.
  
 
شبکه های عصبی بازگشتی مانند نوع ساده آن
 \footnote{\lr{vanilla recurrent neural networks}}
 معمولا در مدل کردن وابستگی های بلند مدت
\footnote{\lr{
long term dependencies
}}
با مشکل مواجه میشوند، این وابستگی ها که در زبان های طبیعی به وفور یافت میشوند باعث شده اند که مدل های زبانی مبتنی بر شبکه های عصبی بازگشتی مدل های چندان خوبی نباشند.
هر چند در انواع دیگر این شبکه ها مانند
\lr{LSTM}
\cite{b4}
 برای حل این مشکل تلاش شده است اما این مشکل هنوز در این نوع از شبکه های عصبی وجود دارد.
 
 \begin{equation*}
 	\label{rnn-formula}
 	h_t = f(W_h h_{t-1} + W_x x_t + b)
 \end{equation*}
 
\subsection{مدل های زبانی مبتنی بر ترنسفورمر}
همان‌طور که در قسمت پیشین اشاره شد، مدل‌های زبانی بازگشتی در مدل‌سازی وابستگی‌های بلندمدت با مشکل مواجه هستند و به دلیل ماهیت توالی‌گونه خود، سرعت پردازش آنها نیز بسیار پایین است.

مقاله 
\lr{Attention is All You Need}
\cite{b5}
 با معماری نوآورانه خود توانست هر دوی این مشکلات را حل کند. این معماری از مکانیزم توجه 
\footnote{\lr{attention mechanism}}
 استفاده می‌کند که به مدل اجازه می‌دهد تا به طور همزمان به تمامی ورودی‌ها توجه کند و وابستگی‌های بلندمدت را به راحتی شناسایی کند. به این ترتیب، سرعت پردازش به طور قابل توجهی افزایش می‌یابد.

علاوه بر این، معماری ترنسفورمر
\footnote{\lr{transformer}}
 که در شکل
 \ref{fig3}
 میتوانید آن را ببینید
 ، قابلیت پردازش موازی را دارد، زیرا معماری آن ماهیت توالی‌گونه ندارد. به همین دلیل، با استفاده از پردازنده‌های گرافیکی
 \footnote{\lr{graphical processing unit}}
  می‌توان پردازش‌های موازی را انجام داد که سرعت پردازش را به طور چشمگیری بهبود می‌بخشد.
  
  مدل های مبتنی بر معماری ترنسفورمر سه دسته هستند که در ادامه به بررسی این سه دسته خواهیم پرداخت.
  
 \subsubsection{مدل‌های زبانی فقط رمزگذار}
مدل‌های زبانی فقط رمزگذار
\footnote{\lr{encoder only language models}}
، تنها از بخش رمزگذار معماری ترنسفورمر که در سمت چپ شکل 
\ref{fig3}
 قابل مشاهده است، استفاده می‌کنند. این مدل‌ها به‌طور ویژه برای پردازش و درک متن طراحی شده‌اند و ورودی‌ها را به یک نمایش داخلی تبدیل می‌کنند که شامل اطلاعات معنایی و ساختاری متن است.

این نوع مدل‌ها عمدتا در وظایفی مانند تحلیل احساسات
 \footnote{\lr{sentiment analysis}}
، دسته‌بندی متن
 \footnote{\lr{text classification}}
 و استخراج ویژگی‌ها
 \footnote{\lr{feature extraction}}
  کاربرد دارند. آن‌ها به‌خوبی می‌توانند الگوهای زبانی و معنایی را شناسایی کرده و اطلاعات مفیدی از داده‌های متنی استخراج کنند، اما به‌تنهایی توانایی تولید متن جدید را ندارند.

به‌عنوان مثال، می‌توان از 
\lr{BERT}
\cite{b6}
\cite{b7}
، که یکی از معروف‌ترین مدل‌های زبانی فقط رمزگذار است، یاد کرد.
 \subsubsection{مدل‌های زبانی فقط رمزگشا}
 مدل‌های زبانی فقط رمزگشا
 \footnote{\lr{decoder-only language models}}
 نوعی از مدل‌های زبانی هستند که به طور خاص برای تولید متن و پیش‌بینی توکن‌های بعدی در یک توالی طراحی شده‌اند. این مدل‌ها تنها از ساختار رمزگشای معماری ترنسفورمر که در سمت راست شکل 
 \ref{fig3}
 قابل مشاهده است،
 استفاده می‌کنند و به صورت تک‌جهته عمل می‌کنند، به این معنا که برای تولید هر توکن، تنها به توکن‌های قبلی خود در توالی دسترسی دارند.
 
 در این مدل‌ها، هدف اصلی پیش‌بینی توکن بعدی بر اساس توکن‌های قبلی است. به عنوان مثال، اگر ورودی مدل یک جمله باشد، مدل سعی می‌کند کلمه بعدی را پیش‌بینی کند. این نوع از مدل‌ها در کاربردهایی مانند تولید متن، چت‌بات‌ها و ترجمه ماشینی بسیار موثر هستند.
 
 مدل‌های رمزگشا معمولا با استفاده از داده‌های متنی بزرگ آموزش دیده و توانایی بالایی در تولید متن‌های معنادار و مرتبط دارند. یکی از معروف‌ترین نمونه‌های این دسته از مدل‌ها، مدل
 \lr{GPT}
 \footnote{\lr{generative pre-trained transformer}}
 \cite{b8}
 است که توسط
 \lr{OpenAI}
  توسعه یافته است.
  \subsubsection{مدل‌های زبانی رمزگذار-رمزگشا}
دل‌های زبانی رمزگذار-رمزگشا 
\footnote{\lr{encoder-decoder language models}}
نوعی از مدل‌های زبانی هستند که هر دو قسمت معماری ترنسفورمر
که در شکل 
 \ref{fig3}
 مشاهده میکنید استفاده میکند.

این معماری که در مدل هایی مانند
\lr{T5}
\cite{b9}
به کار گرفته شده
در وظایف پیچیده‌ای مانند ترجمه ماشینی، خلاصه‌سازی متن و تولید گفتار کاربرد دارد. به عنوان مثال، در ترجمه ماشینی، بخش رمزگذار جمله‌ای را به زبان مبدا تحلیل کرده و آن را به یک نمایش معنایی تبدیل می‌کند، سپس بخش رمزگشا این نمایش را به زبان مقصد ترجمه می‌کند. 
  
 \begin{figure}[ht]
 	\centerline{\includegraphics[width=0.7\textwidth]{fig3}}
 	\caption{
 		معماری ترنسفورمر
 	}
 	\label{fig3}
 \end{figure}
\section{سیستم های پرسش و پاسخ}
سیستم‌های پرسش و پاسخ 
\footnote{\lr{question answering systems}}
	فناوری‌های هوشمندی هستند که با استفاده از تکنیک‌های پردازش زبان طبیعی
	\footnote{\lr{natural language processing}}
	، به کاربران این امکان را می‌دهند تا سوالات خود را  مطرح کرده و پاسخ‌های دقیق و مرتبط دریافت کنند.
	این سیستم ها بر دو نوع هستند استخراجی
	\footnote{\lr{extractive}}
	و تولیدی
	\footnote{\lr{generative}}
	که در ادامه به بررسی آنها خواهیم پرداخت.
	\cite{b10}
\subsection{سیستم های پرسش و پاسخ استخراجی}
 سیستم‌های پرسش و پاسخ استخراجی  به منظور پاسخ‌گویی به سوالات کاربران،به جستجوی اطلاعات در پایگاه‌های داده یا مستندات می‌پردازند و پاسخ‌ها را از متن استخراج می‌کنند.
این سیستم‌ها معمولا از یک مدل زبانی فقط رمزگذار مانند
\lr{bert}
استفاده میکنند.
\subsection{سیستم های پرسش و پاسخ تولیدی}
با توسعه و گسترش هوش مصنوعی تولید کننده
\footnote{\lr{generative artificial inteligence}}
سیستم‌های پرسش و پاسخ‌هایی پدید آمدند که در آن برای پاسخ دادن به پرسش کاربر به دانش مدل زبانی تکیه میشود. یعنی مدل زبانی با توجه به دانشی که در زمینه پرسش مطرح شده دارد بایستی پاسخ را تولید کند.
از آنجایی که در این سیستم‌ها بایستی چیزی تولید شود بنابراین در آن‌ها از مدل‌های زبانی فقط رمزگشا یا مدل‌های زبانی رمزگذار-رمزگشا استفاده میگردد.

با بهبود دانش پزشکی و توانایی استدلال یک مدل پایه ما نیز در پایان نامه حاضر اقدام به طراحی یک سیستم پرسش و پاسخ تولیدی کرده ایم.

\section{شیوه های سنجش مدل های زبانی}
سنجش دانش یک مدل زبانی به ویژه در زمینه پزشکی از اهمیت بالایی برخوردار است، زیرا این فرآیند به شناخت ما از میزان دانش یک مدل زبانی، کمک شایانی می‌کند. 
در ادامه چندین روش سنجش کیفیت پاسخ‌های مدل های زبانی را مطرح خواهیم کرد، ضمنا ما از روش اول و دوم برای سنجش دانش مدل های خود استفاده کرده ایم.
\subsection{سنجش بر اساس میزان پاسخگویی به پرسش و پاسخ های چند گزینه ای}
 یکی از معیارهای مهم برای سنجش عملکرد مدل‌های زبانی، ارزیابی توانایی آن‌ها در پاسخگویی به پرسش‌های چهار گزینه‌ای است که از پیش آماده شده‌اند. این نوع ارزیابی به دلیل ساختار مشخص و استاندارد پرسش‌ها، امکان مقایسه دقیق‌تری بین مدل‌های مختلف را فراهم می‌آورد.
یکی از مجموعه‌های داده‌ای که به طور گسترده در این زمینه مورد استفاده قرار می‌گیرد، مجموعه داده 
\lr{MMLU}
\footnote{\lr{massive multitask language understanding}}
\cite{b11}
است. این مجموعه شامل پرسش‌های متنوعی است که درباره موضوعات مختلفی مانند علوم،پزشکی، ریاضیات، تاریخ، و ادبیات طراحی شده‌اند. 

مجموعه داده
\lr{MMLU}
به عنوان یک استاندارد در ارزیابی مدل‌های زبانی، به محققان و توسعه‌دهندگان این امکان را می‌دهد که عملکرد مدل‌ های زبانی خود را در زمینه‌های مختلف بسنجند و نقاط قوت و ضعف آن‌ها را شناسایی کنند.
پرسش‌های چهار گزینه‌ای در
\lr{MMLU}
به گونه‌ای طراحی شده‌اند که نیاز به درک عمیق و تحلیل دقیق متن دارند. این ویژگی، مدل‌ها را به چالش می‌کشد تا نه تنها اطلاعات را بازیابی کنند، بلکه توانایی استدلال و تحلیل خود را نیز به نمایش بگذارند.
با استفاده از این معیار، می‌توان به راحتی مقایسه‌هایی بین مدل‌های مختلف انجام داد و پیشرفت‌های حاصل شده در زمینه هوش مصنوعی و پردازش زبان طبیعی را ارزیابی کرد.
\subsection{سنجش بر اساس نظر مدل داور}
یک روش دیگر برای سنجش عملکرد یک مدل زبانی، استفاده از یک مدل زبانی دیگر به عنوان داور است. در این رویکرد، یک مدل زبانی مستقل به عنوان مرجع برای ارزیابی کیفیت پاسخ‌های تولید شده توسط مدل اصلی مورد استفاده قرار می‌گیرد. این روش به دلیل قابلیت‌های بالای مدل‌های زبانی در پردازش و درک زبان طبیعی، می‌تواند به طور موثری به ارزیابی دقت و کیفیت پاسخ‌ها کمک کند.

در این فرآیند، پاسخ‌های تولید شده توسط مدل اصلی به مدل قاضی ارائه می‌شود. مدل قاضی می‌تواند با استفاده از معیارهای مختلفی مانند شباهت معنایی با پاسخ اصلی
\footnote{\lr{ground truth}}
، صحت اطلاعات، و سازگاری با زمینه، کیفیت پاسخ‌ها را ارزیابی کند. به عنوان مثال، مدل قاضی می‌تواند با بررسی تطابق پاسخ‌ها با اطلاعات موجود در متون معتبر یا داده‌های آموزشی، نمره‌ای برای هر پاسخ تولید کرده
\cite{b12}
یا پاسخی را بر پاسخ دیگر ترجیح دهد.
\subsection{سنجش بر اساس استنتاج زبان طبیعی}
با داشتن یک مجموعه داده مانند 
\lr{K-QA} 
\cite{b13}
، شامل پاسخ‌های تولید شده توسط انسان که به همراه توضیحات دقیقی دسته‌بندی شده‌اند، این پاسخ‌ها به عنوان "الزامی"
\footnote{\lr{must have}}
 یا "مفید"
 \footnote{\lr{nice to have}} 
 مشخص گشته‌اند. این دسته‌بندی نشان می‌دهد که آیا توضیحات باید به طور ضروری در پاسخ گنجانده شوند یا اینکه اضافی و مفید هستند.

این حقایق اتمی می‌توانند برای به کارگیری یک روش ارزیابی مبتنی بر استنتاج زبان طبیعی 
\footnote{\lr{natural language inference}} 
استفاده شوند. در این روش، پاسخ مدل به عنوان "مقدمه" 
\footnote{\lr{premise}} 
و هر یک از توضیحات انسانی به عنوان "فرضیه" 
\footnote{\lr{hypothesis}}
 در نظر گرفته می‌شود. سپس یک مدل زبانی توانا در حوزه استنتاج زبان طبیعی تعیین خواهد کرد که آیا مقدمه مستلزم 
 \footnote{\lr{entailment}}
، متناقض 
 \footnote{\lr{contradiction}}
  یا خنثی 
 \footnote{\lr{neutral}} 
 با فرضیه است.

با انجام این کار روی همه رکوردهای مجموعه داده، دو امتیاز کامل بودن 
\footnote{\lr{completeness}}
 و حقیقت داشتن
  \footnote{\lr{factuality}} 
 به صورت زیر به دست می‌آید.
 
\begin{equation*}
	\label{completeness-formula}
	S_{comp}(r_i,A_i^{\prime}) = \frac{1[r_i \quad entails \quad a]}{|A_i^{\prime}|}
\end{equation*}

\begin{equation*}
	\label{factuality-formula}
	S_{fact}(r_i,A_i^{\prime}) = \begin{cases}
		0 & \text{if } \quad \exists a \in A_i \quad s.t. \quad r_i  \quad contradicts \quad a \\
		1 & \text{if } \quad otherwise
	\end{cases}
\end{equation*}




\subsection{سنجش بر اساس امتیاز 
\lr{bert}
}
در این معیار، کیفیت پاسخ‌های تولید شده توسط یک مدل زبانی فقط رمزگذار مانند
\lr{bert} 
ارزیابی می‌شود.
\cite{b14}
در این فرآیند، پاسخ داده شده با پاسخ صحیح
\footnote{\lr{ground truth}}
 موجود در مجموعه داده مقایسه می‌گردد.

مدل زبانی پاسخ فقط رمزگذار را به دو بردار تبدیل می‌کند: یکی برای پاسخ داده شده و دیگری برای پاسخ صحیح. سپس برای محاسبه امتیاز 
\lr{bert}
، کافی است میزان شباهت این دو بردار را با یک روش شباهت‌سنجی مانند شباهت کسینوسی
\footnote{\lr{cosine similarity}}
 محاسبه کنیم.

پس از محاسبه امتیازها برای تمامی پاسخ‌ها در مجموعه داده، این امتیازات جمع‌آوری و میانگین‌گیری می‌شوند. میانگین امتیازها نمای کلی از عملکرد مدل را ارائه می‌دهد و به شناسایی نقاط قوت و ضعف آن کمک می‌کند.

\section{
یادگیری تقویتی در مدل های زبانی
}
با داشتن یک مدل زبانی اولیه که بتواند برای هر پرسشی پاسخ خود را تولید کند، می‌توانیم یک مدل یا تابع پاداش
\footnote{\lr{reward model or function}}
در نظر بگیریم و آن مدل زبانی را به‌گونه‌ای به‌روزرسانی کنیم که این تابع پاداش بیشینه گردد. به این صورت که ابتدا داده‌های ترجیحات انسانی (مانند مقایسه دو پاسخ و انتخاب بهتر) جمع‌آوری شده و با آن‌ها یک مدل پاداش آموزش داده می‌شود. سپس، با استفاده از الگوریتم‌های بهینه سازی که در ادامه به آن ها خواهیم پرداخت، سیاست تولید پاسخ مدل زبانی به‌تدریج اصلاح می‌گردد تا امتیاز پاداش بالاتری کسب کند. این شیوه باعث می‌شود که مدل زبانی نه‌تنها با ترجیحات انسانی هم‌راستا شود، بلکه پاسخ‌های مخرب، نادرست، جانبدارانه یا نامناسب کاهش یابد، ایمنی و قابلیت اطمینان افزایش یابد و کیفیت کلی مکالمه از نظر روانی بودن، مفید بودن و جذابیت به‌طور چشمگیری بهبود یابد. در نهایت، این روش به مدل‌های زبانی کمک می‌کند تا خروجی‌هایی تولید کنند که نه‌تنها دقیق و مرتبط باشند، بلکه با ارزش‌ها، فرهنگ و انتظارات کاربران انسانی نیز هماهنگ گردند.
\subsection{
یادگیری تقویتی با بازخورد انسان یا هوش مصنوعی
}
برای مشخص کردن ترجیحات انسانی یاد شده علاوه بر انسان
\footnote{\lr{reinforcement learning with human feedback}}
\cite{b15}
می‌توان از بازخورد هوش مصنوعی نیز استفاده کرد؛ بدین صورت که از یک مدل هوش مصنوعی پیشرفته‌تر یا تخصصی (مانند یک مدل زبانی بزرگ‌تر یا یک سیستم ارزیابی خودکار) خواسته می‌شود که به پاسخی که یک مدل زبانی تولید کرده نمره‌ای عددی (مثلا از ۱ تا ۱۰ بر اساس معیارهایی نظیر صحت، مفید بودن، روانی و ایمنی) بدهد یا آن را در قیاس با پاسخ‌های دیگر تولیدشده توسط همان مدل یا مدل‌های متفاوت رتبه‌بندی کند. این روش که از آن با عنوان یادگیری تقویتی با بازخورد هوش مصنوعی
\cite{b16}
\footnote{\lr{reinforcement learning with AI feedback}}
یاد میشود، هزینه‌های جمع‌آوری داده‌های انسانی را به شدت کاهش می‌دهد، سرعت فرآیند را افزایش می‌دهد و امکان مقیاس‌پذیری گسترده‌تری فراهم می‌آورد، هرچند ممکن است به دلیل محدودیت‌های مدل ارزیابی‌کننده، برخی جنبه‌های ظریف فرهنگی، اخلاقی یا زمینه‌ای انسانی را به‌طور کامل پوشش ندهد؛ بنابراین اغلب ترکیبی از بازخورد انسانی و هوش مصنوعی برای دستیابی به بهترین نتایج به کار گرفته می‌شود.
\subsection{
الگوریتم های بهینه سازی مدل زبانی
}
الگوریتم‌های متعددی برای بهبود عملکرد مدل‌های زبانی بزرگ پیشنهاد شده‌اند که هر یک از رویکردهای متفاوتی در چارچوب یادگیری تقویتی
\footnote{\lr{reinforcement learning}}
بهره می‌برند. از میان این روش‌ها، می‌توان به الگوریتم بهینه‌سازی سیاست مجاورتی
\footnote{\lr{proximal policy optimization}}
\cite{b17}
به‌عنوان یکی از پرکاربردترین و پایدارترین روش‌ها اشاره کرد. این الگوریتم با محدود کردن میزان تغییر در سیاست، موجب پایداری در فرآیند آموزش می‌شود. در این رویکرد، ابتدا یک مدل پاداش آموزش داده می‌شود و سپس مدل زبانی تلاش می‌کند تا با بیشینه‌سازی مقدار خروجی این مدل پاداش، عملکرد خود را بهبود دهد.

یکی دیگر از این الگوریتم‌ها، بهینه‌سازی سیاست بر پایه پاداش گروهی
\footnote{\lr{group reward policy optimization}}
\cite{b18}
است. این الگوریتم با معرفی پاداش‌های نسبی درون‌گروهی، تلاش می‌کند مشکلات نوسان و ناپایداری موجود در الگوریتم بهینه‌سازی سیاست مجاورتی را کاهش دهد و از طریق ارزیابی نسبی پاسخ‌ها در یک گروه، بازخورد دقیق‌تر و پایدار‌تری برای آموزش فراهم کند.

در ادامه، الگوریتم بهینه‌سازی مستقیم ترجیحات
\footnote{\lr{direct preference optimization}}
\cite{b19}
به جای تکیه بر یک مدل پاداش بیرونی، سعی دارد تابع هدف درونی
\footnote{\lr{intrinsic objective function}}
را در مدل زبانی بهینه کند. همانطور که در تصویر
\ref{fig8}
میبینید در این روش، مدل به‌صورت مستقیم از جفت پاسخ‌های ترجیحی (پاسخ تاییدشده و پاسخ ردشده)
\footnote{\lr{rejected–preferred answer pairs}}
یاد می‌گیرد و نیازی به تخمین یا آموزش مدل پاداش مجزا ندارد. این ویژگی باعث سادگی و پایداری بیشتر در فرآیند هم‌ترازسازی مدل با ترجیحات انسانی می‌شود.
تابع هزینه این الگوریتم به‌صورت زیر تعریف می‌شود

\begin{equation}
\mathcal{L}_{\text{DPO}}(\theta) = 
- \mathbb{E}_{(x, y^+, y^-)} 
\left[
\log \sigma\left(
\beta \left(
\log \frac{\pi_\theta(y^+|x)}{\pi_{\text{ref}}(y^+|x)}
-
\log \frac{\pi_\theta(y^-|x)}{\pi_{\text{ref}}(y^-|x)}
\right)
\right)
\right]
\end{equation}


 \begin{figure}[ht]
 	\centerline{\includegraphics[width=0.7\textwidth]{fig8}}
 	\caption{
 		مقایسه الگوریتم بهینه‌سازی سیاست مجاورتی و بهینه‌سازی مستقیم ترجیحات
 	}
 	\label{fig8}
 \end{figure}


در نهایت، روش بهینه‌سازی مستقیم تابع کیو
\footnote{\lr{direct q-function optimization}}
\cite{b20}
فرآیند تولید پاسخ را به‌صورت یک فرآیند تصمیم مارکوف
\footnote{\lr{markov decision process}}
مدل می‌کند و با بهره‌گیری از چارچوب بازیگر–منتقد نرم
\footnote{\lr{soft actor-critic}}
\cite{b21}
به‌طور مستقیم تابع 
\lr{Q}
 را که به‌وسیله مدل زبانی پارامترگذاری شده است، بهینه می‌سازد. این مدل‌سازی بر پایه‌ی زنجیره تصمیم گیری مارکف، نسبت به روش‌های مبتنی بر 
\lr{Bandit}
از ساختار قوی‌تری برخوردار است و امکان نظارت فرایندی مؤثرتر را فراهم می‌کند.

به طور کلی، هر یک از این الگوریتم‌ها با هدف افزایش کارایی، پایداری و هم‌خوانی رفتاری مدل‌های زبانی با ترجیحات انسانی توسعه یافته‌اند و بخش مهمی از پیشرفت‌های اخیر در حوزه‌ی آموزش و هم‌ترازسازی مدل‌های زبانی را تشکیل می‌دهند.
