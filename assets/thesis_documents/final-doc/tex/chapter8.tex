% !TeX root=../main.tex

\chapter{نتیجه گیری}

\begin{thebibliography}{00}
	\bibitem{b1}
	\lr{Brown, Peter F., et al. "A statistical approach to machine translation." (1990): 79-85.}
	\bibitem{b2}
	\lr{Williams, Ronald J., and David Zipser. "A learning algorithm for continually running fully recurrent neural networks." Neural computation 1.2 (1989): 270-280.}
	\bibitem{b3}
	\lr{Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780.}
	\bibitem{b4} 
	\lr{Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).}
	\bibitem{b5}
	\lr{Koroteev, Mikhail V. "BERT: a review of applications in natural language processing and understanding." arXiv preprint arXiv:2103.11943 (2021).}
	\bibitem{b6}
	\lr{Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019.}
	\bibitem{b7}
	\lr{Yenduri, Gokul, et al. "Generative pre-trained transformer: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions." arXiv preprint arXiv:2305.10435 (2023).}
	\bibitem{b8}
	\lr{Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." Journal of machine learning research 21.140 (2020): 1-67.}
	\bibitem{b9}
	\lr{
	Luo, Man, et al. "Choose your QA model wisely: A systematic study of generative and extractive readers for question answering." arXiv preprint arXiv:2203.07522 (2022).
	}
	\bibitem{b10}
	\lr{
	Hendrycks, Dan, et al. "Measuring massive multitask language understanding." arXiv preprint arXiv:2009.03300 (2020).
	}
	\bibitem{b11} 
	\lr{Liu, Yang, et al. "G-eval: NLG evaluation using gpt-4 with better human alignment." arXiv preprint arXiv:2303.16634 (2023).}
	\bibitem{b12}
	\lr{Manes, Itay, et al. "K-qa: A real-world medical q\&a benchmark." arXiv preprint arXiv:2401.14493 (2024).}
	\bibitem{b13} 
	\lr{Singhal, Karan, et al. "Toward expert-level medical question answering with large language models." Nature Medicine (2025): 1-8.}
	\bibitem{b14}
	\lr{Singhal, Karan, et al. "Large language models encode clinical knowledge." Nature 620.7972 (2023): 172-180.}
	\bibitem{b15}
	\lr{
	Saab, Khaled, et al. "Capabilities of gemini models in medicine." arXiv preprint arXiv:2404.18416 (2024).
	}
	\bibitem{b16} 
	\lr{Li, Yunxiang, et al. "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge." Cureus 15.6 (2023).}
	\bibitem{b17} 
	\lr{Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." arXiv preprint arXiv:2302.13971 (2023).}
	\bibitem{b18} 
	\lr{Kim, Hyunjae, et al. "Small language models learn enhanced reasoning skills from medical textbooks." arXiv preprint arXiv:2404.00376 (2024).}
	\bibitem{b19} 
	\lr{Vishwanath, Krithik, et al. "MedMobile: A mobile-sized language model with expert-level clinical capabilities." arXiv preprint arXiv:2410.09019 (2024).}
	\bibitem{b20} 
	\lr{Abdin, Marah, et al. "Phi-3 technical report: A highly capable language model locally on your phone." /
		arXiv preprint arXiv:2404.14219 (2024).}
	\bibitem{b21} 
	\lr{Taghizadeh, Nasrin, et al. "SINA-BERT: a pre-trained language model for analysis of medical texts in Persian." arXiv preprint arXiv:2104.07613 (2021).}
	\bibitem{b22} 
	\lr{Koroteev, Mikhail V. "BERT: a review of applications in natural language processing and understanding." arXiv preprint arXiv:2103.11943 (2021).}
	\bibitem{b23}
	\lr{Veisi, Hadi, and Hamed Fakour Shandi. "A Persian medical question answering system." International Journal on Artificial Intelligence Tools 29.06 (2020): 2050019.}
	\bibitem{b24}
	\lr{Darabi, Leila. Medical Question Answering for Persian. Master’s thesis, LIACS, Leiden University, 2024.}
	\bibitem{b25}
	\lr{Farahani, Mehrdad, et al. "Parsbert: Transformer-based model for persian language understanding." Neural Processing Letters 53 (2021): 3831-3847.}
	\bibitem{b26}
	\lr{García-Ferrero, Iker, et al. "Medical mT5: an open-source multilingual text-to-text LLM for the medical domain." arXiv preprint arXiv:2404.07613 (2024).}
	\bibitem{b27}
	\lr{Liu, Yang, et al. "Datasets for large language models: A comprehensive survey." arXiv preprint arXiv:2402.18041 (2024).}
	\bibitem{b28}
	\lr{Yang, Songhua, et al. "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue." Proceedings of the AAAI conference on artificial intelligence. Vol. 38. No. 17. 2024.}
	\bibitem{b29}
	\lr{Bao, Zhijie, et al. "Disc-medllm: Bridging general large language models and real-world medical consultation." arXiv preprint arXiv:2308.14346 (2023).}
	\bibitem{b30}
	\lr{Zhang, Hongbo, et al. "Huatuogpt, towards taming language model to be a doctor." arXiv preprint arXiv:2305.15075 (2023).}
	\bibitem{b31}
	\lr{Wang, Xidong, et al. "Huatuo-26M, a Large-scale Chinese Medical QA Dataset." Findings of the Association for Computational Linguistics: NAACL 2025. 2025.}
	\bibitem{b32}
	\lr{Zeng, Guangtao, et al. "MedDialog: Large-scale medical dialogue datasets." Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP). 2020.}
	\bibitem{b33}
	\lr{Han, Tianyu, et al. "MedAlpaca--an open-source collection of medical conversational AI models and training data." arXiv preprint arXiv:2304.08247 (2023).}
	\bibitem{b34}
	\lr{Yang, An, et al. “Qwen2 Technical Report.” arXiv Preprint arXiv:2407.10671, 2024.}
	\bibitem{b35} 
	\lr{Dang, John, et al. "Aya expanse: Combining research breakthroughs for a new multilingual frontier." arXiv preprint arXiv:2412.04261 (2024).}
	\bibitem{b36} 
	\lr{Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size, 2024." URL https://arxiv. org/abs/2408.00118 1.3 (2024).}
	\bibitem{b37} 
	\lr{Rostami, Pedram, Ali Salemi, and Mohammad Javad Dousti. "Persianmind: A cross-lingual persian-english large language model." arXiv preprint arXiv:2401.06466 (2024).}
	\bibitem{b38}
	\lr{Dash, Saurabh, et al. "Aya Vision: Advancing the Frontier of Multilingual Multimodality." arXiv preprint arXiv:2505.08751 (2025).}
	\bibitem{b39}
	\lr{Shumailov, Ilia, et al. "AI models collapse when trained on recursively generated data." Nature 631.8022 (2024): 755-759.}
	\bibitem{b40}
	\lr{
	Odumakinde, Ayomide, et al. "Multilingual arbitrage: Optimizing data pools to accelerate multilingual progress, 2024." URL https://arxiv.org/abs/2408.14960.
	}
	\bibitem{b41}
	\lr{
	Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." ICLR 1.2 (2022): 3.
	}
	\bibitem{b42}
	\lr{
	Dao, Tri. "Flashattention-2: Faster attention with better parallelism and work partitioning." arXiv preprint arXiv:2307.08691 (2023).
	}
	\bibitem{b43}
	\lr{
	Lacoste, Alexandre, et al. "Quantifying the carbon emissions of machine learning." arXiv preprint arXiv:1910.09700 (2019).
	}
	\bibitem{b44}
	\lr{
	Hurst, Aaron, et al. "Gpt-4o system card." arXiv preprint arXiv:2410.21276 (2024).
	}
	\bibitem{b45}
	\lr{
	Khashabi, Daniel, et al. "Parsinlu: a suite of language understanding challenges for persian." Transactions of the Association for Computational Linguistics 9 (2021): 1147-1162.
	}
	\bibitem{b46}
	\lr{
	Kashefi, Omid. "MIZAN: a large persian-english parallel corpus." arXiv preprint arXiv:1801.02107 (2018).
	}
\end{thebibliography}