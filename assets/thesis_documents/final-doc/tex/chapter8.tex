% !TeX root=../main.tex

\chapter{نتیجه گیری}

\begin{thebibliography}{00}
        \bibitem{b1}
        Momani, Ahmad. "Implications of Artificial Intelligence on Health Data Privacy and Confidentiality." arXiv preprint arXiv:2501.01639 (2025).‏
	\bibitem{b2}
	\lr{Brown, Peter F., et al. "A statistical approach to machine translation." (1990): 79-85.}
	\bibitem{b3}
	\lr{Williams, Ronald J., and David Zipser. "A learning algorithm for continually running fully recurrent neural networks." Neural computation 1.2 (1989): 270-280.}
	\bibitem{b4}
	\lr{Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780.}
	\bibitem{b5} 
	\lr{Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).}
	\bibitem{b6}
	\lr{Koroteev, Mikhail V. "BERT: a review of applications in natural language processing and understanding." arXiv preprint arXiv:2103.11943 (2021).}
	\bibitem{b7}
	\lr{Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019.}
	\bibitem{b8}
	\lr{Yenduri, Gokul, et al. "Generative pre-trained transformer: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions." arXiv preprint arXiv:2305.10435 (2023).}
	\bibitem{b9}
	\lr{Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." Journal of machine learning research 21.140 (2020): 1-67.}
	\bibitem{b10}
	\lr{Luo, Man, et al. "Choose your QA model wisely: A systematic study of generative and extractive readers for question answering." arXiv preprint arXiv:2203.07522 (2022).}
	\bibitem{b11}
	\lr{Hendrycks, Dan, et al. "Measuring massive multitask language understanding." arXiv preprint arXiv:2009.03300 (2020).}
	\bibitem{b12} 
	\lr{Liu, Yang, et al. "G-eval: NLG evaluation using gpt-4 with better human alignment." arXiv preprint arXiv:2303.16634 (2023).}
	\bibitem{b13}
	\lr{Manes, Itay, et al. "K-qa: A real-world medical q\&a benchmark." arXiv preprint arXiv:2401.14493 (2024).}
	\bibitem{b14}
	\lr{Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." arXiv preprint arXiv:1904.09675 (2019).‏}
	\bibitem{b15}
	\lr{Christiano, Paul F., et al. "Deep reinforcement learning from human preferences." Advances in neural information processing systems 30 (2017).‏}
	\bibitem{b16}
	\lr{Lee, Harrison, et al. "Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback." arXiv preprint arXiv:2309.00267 (2023).‏}
	\bibitem{b17}
	\lr{Schulman, John, et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).‏}
	\bibitem{b18}
	\lr{Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).‏}
	\bibitem{b19}
	\lr{Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.‏}
	\bibitem{b20}
	\lr{Ji, Kaixuan, et al. "Enhancing multi-step reasoning abilities of language models through direct q-function optimization." arXiv preprint arXiv:2410.09302 (2024).‏}
	\bibitem{b21}
	\lr{Haarnoja, Tuomas, et al. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." International conference on machine learning. Pmlr, 2018.‏}
	\bibitem{b22} 
	\lr{Singhal, Karan, et al. "Toward expert-level medical question answering with large language models." Nature Medicine (2025): 1-8.}
	\bibitem{b23}
	\lr{Singhal, Karan, et al. "Large language models encode clinical knowledge." Nature 620.7972 (2023): 172-180.}
	\bibitem{b24}
	\lr{Saab, Khaled, et al. "Capabilities of gemini models in medicine." arXiv preprint arXiv:2404.18416 (2024).}
	\bibitem{b25} 
	\lr{Li, Yunxiang, et al. "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge." Cureus 15.6 (2023).}
	\bibitem{b26} 
	\lr{Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." arXiv preprint arXiv:2302.13971 (2023).}
	\bibitem{b27} 
	\lr{Kim, Hyunjae, et al. "Small language models learn enhanced reasoning skills from medical textbooks." arXiv preprint arXiv:2404.00376 (2024).}
	\bibitem{b28} 
	\lr{Vishwanath, Krithik, et al. "MedMobile: A mobile-sized language model with expert-level clinical capabilities." arXiv preprint arXiv:2410.09019 (2024).}
	\bibitem{b29} 
	\lr{Abdin, Marah, et al. "Phi-3 technical report: A highly capable language model locally on your phone." arXiv preprint arXiv:2404.14219 (2024).}
	\bibitem{b30} 
	\lr{Taghizadeh, Nasrin, et al. "SINA-BERT: a pre-trained language model for analysis of medical texts in Persian." arXiv preprint arXiv:2104.07613 (2021).}
	\bibitem{b31} 
	\lr{Koroteev, Mikhail V. "BERT: a review of applications in natural language processing and understanding." arXiv preprint arXiv:2103.11943 (2021).}
	\bibitem{b32}
	\lr{Veisi, Hadi, and Hamed Fakour Shandi. "A Persian medical question answering system." International Journal on Artificial Intelligence Tools 29.06 (2020): 2050019.}
	\bibitem{b33}
	\lr{Darabi, Leila. Medical Question Answering for Persian. Master’s thesis, LIACS, Leiden University, 2024.}
	\bibitem{b34}
	\lr{Farahani, Mehrdad, et al. "Parsbert: Transformer-based model for persian language understanding." Neural Processing Letters 53 (2021): 3831-3847.}
	\bibitem{b35}
	\lr{Jiang, Shuyang, et al. "MedS $^ 3$: Towards Medical Slow Thinking with Self-Evolved Soft Dual-sided Process Supervision." arXiv preprint arXiv:2501.12051 (2025).‏}
	\bibitem{b36}
	\lr{Coulom, Rémi. "Efficient selectivity and backup operators in Monte-Carlo tree search." International conference on computers and games. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006.‏}
	\bibitem{b37}
	\lr{Wu, Juncheng, et al. "Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs." arXiv preprint arXiv:2504.00993 (2025).‏}
	\bibitem{b38}
	\lr{Chen, Junying, et al. "Huatuogpt-o1, towards medical complex reasoning with llms." arXiv preprint arXiv:2412.18925 (2024).‏}
	\bibitem{b39}
	\lr{Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).‏}
	\bibitem{b40}
	\lr{Wu, Tianhao, et al. "Thinking llms: General instruction following with thought generation." arXiv preprint arXiv:2410.10630 (2024).‏}
	\bibitem{b41}
	\lr{Ho, Namgyu, Laura Schmid, and Se-Young Yun. "Large language models are reasoning teachers." Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers). 2023.}‏
	\bibitem{b42}
	\lr{García-Ferrero, Iker, et al. "Medical mT5: an open-source multilingual text-to-text LLM for the medical domain." arXiv preprint arXiv:2404.07613 (2024).}
	\bibitem{b43}
	\lr{Liu, Yang, et al. "Datasets for large language models: A comprehensive survey." arXiv preprint arXiv:2402.18041 (2024).}
	\bibitem{b44}
	\lr{Yang, Songhua, et al. "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue." Proceedings of the AAAI conference on artificial intelligence. Vol. 38. No. 17. 2024.}
	\bibitem{b45}
	\lr{Bao, Zhijie, et al. "Disc-medllm: Bridging general large language models and real-world medical consultation." arXiv preprint arXiv:2308.14346 (2023).}
	\bibitem{b46}
	\lr{Zhang, Hongbo, et al. "Huatuogpt, towards taming language model to be a doctor." arXiv preprint arXiv:2305.15075 (2023).}
	\bibitem{b47}
	\lr{Wang, Xidong, et al. "Huatuo-26M, a Large-scale Chinese Medical QA Dataset." Findings of the Association for Computational Linguistics: NAACL 2025. 2025.}
	\bibitem{b48}
	\lr{Zeng, Guangtao, et al. "MedDialog: Large-scale medical dialogue datasets." Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP). 2020.}
	\bibitem{b49}
	\lr{Han, Tianyu, et al. "MedAlpaca--an open-source collection of medical conversational AI models and training data." arXiv preprint arXiv:2304.08247 (2023).}
	\bibitem{b50}
	\lr{Bi, Xiao, et al. "Deepseek llm: Scaling open-source language models with longtermism." arXiv preprint arXiv:2401.02954 (2024).‏}
	\bibitem{b51}
	\lr{Pal, Ankit, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering." Conference on health, inference, and learning. PMLR, 2022.‏}
	\bibitem{b52}
	\lr{C-LARA-Instance, Manny Rayner ChatGPT. "How Woke is Grok? Empirical Evidence that xAI’s Grok Aligns Closely with Other Frontier Models." (2025).‏}
	\bibitem{b53}
	\lr{Achiam, Josh, et al. "Gpt-4 technical report." arXiv preprint arXiv:2303.08774 (2023).‏}
	\bibitem{b54}
	\lr{Jamali, Naghmeh, et al. "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language." arXiv preprint arXiv:2505.18331 (2025).‏}
	\bibitem{b55}
	\lr{Kalahroodi, Mohammad Javad Ranjbar, et al. "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain." arXiv preprint arXiv:2506.00250 (2025).‏}
	\bibitem{b56}
	\lr{Yang, An, et al. “Qwen2 Technical Report.” arXiv Preprint arXiv:2407.10671, 2024.}
	\bibitem{b57} 
	\lr{Dang, John, et al. "Aya expanse: Combining research breakthroughs for a new multilingual frontier." arXiv preprint arXiv:2412.04261 (2024).}
	\bibitem{b58} 
	\lr{Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size, 2024." URL https://arxiv. org/abs/2408.00118 1.3 (2024).}
	\bibitem{b59} 
	\lr{Rostami, Pedram, Ali Salemi, and Mohammad Javad Dousti. "Persianmind: A cross-lingual persian-english large language model." arXiv preprint arXiv:2401.06466 (2024).}
	\bibitem{b60}
	\lr{Dash, Saurabh, et al. "Aya Vision: Advancing the Frontier of Multilingual Multimodality." arXiv preprint arXiv:2505.08751 (2025).}
	\bibitem{b61}
	\lr{Shumailov, Ilia, et al. "AI models collapse when trained on recursively generated data." Nature 631.8022 (2024): 755-759.}
	\bibitem{b62}
	\lr{Odumakinde, Ayomide, et al. "Multilingual arbitrage: Optimizing data pools to accelerate multilingual progress, 2024." URL https://arxiv.org/abs/2408.14960.}
	\bibitem{b63}
	\lr{Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." ICLR 1.2 (2022): 3.}
	\bibitem{b64}
	\lr{Dao, Tri. "Flashattention-2: Faster attention with better parallelism and work partitioning." arXiv preprint arXiv:2307.08691 (2023).}
	\bibitem{b65}
	\lr{Lacoste, Alexandre, et al. "Quantifying the carbon emissions of machine learning." arXiv preprint arXiv:1910.09700 (2019).}
	\bibitem{b66}
	\lr{Hurst, Aaron, et al. "Gpt-4o system card." arXiv preprint arXiv:2410.21276 (2024).}
	\bibitem{b67}
	\lr{Khashabi, Daniel, et al. "Parsinlu: a suite of language understanding challenges for persian." Transactions of the Association for Computational Linguistics 9 (2021): 1147-1162.}
	\bibitem{b68}
	\lr{Kashefi, Omid. "MIZAN: a large persian-english parallel corpus." arXiv preprint arXiv:1801.02107 (2018).}
	\bibitem{b69}
	\lr{Bengio, Yoshua. "From system 1 deep learning to system 2 deep learning." Neural Information Processing Systems. 2019.‏}
	\bibitem{b70}
	\lr{Kahneman, Daniel. "Thinking, fast and slow." Farrar, Straus and Giroux (2011).‏}
	\bibitem{b71}
	\lr{Goyal, Anirudh, et al. "Recurrent independent mechanisms." arXiv preprint arXiv:1909.10893 (2019).‏}
	\bibitem{b72}
	\lr{Wang, Guan, et al. "Hierarchical Reasoning Model." arXiv preprint arXiv:2506.21734 (2025).‏‏}
	\bibitem{b73}
	\lr{Pan, Liangming, et al. "Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies." arXiv preprint arXiv:2308.03188 (2023).‏}
	\bibitem{b74}
	\lr{Liang, Baoyu, Yuchen Wang, and Chao Tong. "AI Reasoning in Deep Learning Era: From Symbolic AI to Neural–Symbolic AI." Mathematics 13.11 (2025): 1707.‏}
	\bibitem{b75}
	\lr{Lightman, Hunter, et al. "Let's verify step by step." The Twelfth International Conference on Learning Representations. 2023.}
	\bibitem{b76}
	\lr{Huang, Jie, and Kevin Chen-Chuan Chang. "Towards reasoning in large language models: A survey." Findings of the association for computational linguistics: ACL 2023. 2023.‏}
	\bibitem{b77}
	\lr{Shojaee, Parshin, et al. "The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity." arXiv preprint arXiv:2506.06941 (2025).‏}
	\bibitem{b78}
	\lr{Wei, Jason, et al. "Emergent abilities of large language models." arXiv preprint arXiv:2206.07682 (2022).‏}
	\bibitem{b79}
	\lr{Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.‏‏}
	\bibitem{b80}
	\lr{Brown, Bradley, et al. "Large language monkeys: Scaling inference compute with repeated sampling." arXiv preprint arXiv:2407.21787 (2024).‏}
	\bibitem{b81}
	\lr{Zhang, Mozhi, et al. "Calibrating the confidence of large language models by eliciting fidelity." arXiv preprint arXiv:2404.02655 (2024).‏}
\end{thebibliography}
